{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a68417-b7dd-4153-995c-a08157cfcd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7184f00d-944d-4c52-a948-35ca0bba0e13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu126 for torchao version 0.14.0         Please see GitHub issue #2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-05 17:45:53 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-05 17:46:04 [config.py:1604] Using max model len 31000\n",
      "WARNING 11-05 17:46:04 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 11-05 17:46:04 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/data01/kilm/users/quocvh/HF/models/Qwen/Qwen3-4B-Instruct-2507', speculative_config=None, tokenizer='/data01/kilm/users/quocvh/HF/models/Qwen/Qwen3-4B-Instruct-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=31000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/data01/kilm/users/quocvh/HF/models/Qwen/Qwen3-4B-Instruct-2507, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 11-05 17:46:05 [cuda.py:398] Using Flash Attention backend.\n",
      "INFO 11-05 17:46:05 [parallel_state.py:1102] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-05 17:46:05 [model_runner.py:1083] Starting to load model /data01/kilm/users/quocvh/HF/models/Qwen/Qwen3-4B-Instruct-2507...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a2f49de9ca4ef290205300fdd684ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-05 17:46:10 [default_loader.py:262] Loading weights took 1.56 seconds\n",
      "INFO 11-05 17:46:11 [model_runner.py:1115] Model loading took 7.6065 GiB and 4.705542 seconds\n",
      "INFO 11-05 17:46:17 [worker.py:295] Memory profiling takes 5.12 seconds\n",
      "INFO 11-05 17:46:17 [worker.py:295] the current vLLM instance can use total_gpu_memory (23.50GiB) x gpu_memory_utilization (0.85) = 19.97GiB\n",
      "INFO 11-05 17:46:17 [worker.py:295] model weights take 7.61GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 2.29GiB; the rest of the memory reserved for KV Cache is 10.02GiB.\n",
      "INFO 11-05 17:46:17 [executor_base.py:113] # cuda blocks: 4561, # CPU blocks: 1820\n",
      "INFO 11-05 17:46:17 [executor_base.py:118] Maximum concurrency for 31000 tokens per request: 2.35x\n",
      "INFO 11-05 17:46:25 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 14.04 seconds\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# llm = LLM(model=\"/mnt/nfs-data/kilm-storage/public-llm/Qwen2.5-3B-Instruct\", gpu_memory_utilization=0.95)\n",
    "# llm = LLM(model=\"/mnt/nfs-data/kilm-storage/public-llm/Qwen2.5-3B-Instruct\", gpu_memory_utilization=0.9)\n",
    "\n",
    "llm = LLM(\n",
    "    model=\"/data01/kilm/users/quocvh/HF/models/Qwen/Qwen3-4B-Instruct-2507\",\n",
    "    gpu_memory_utilization=0.85,\n",
    "    max_model_len=31000,\n",
    "    enforce_eager=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d61dec7b-aff7-4b90-ba61-b679e2ee0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# SET = \"private\"\n",
    "SET = \"public\"\n",
    "# df = pd.read_csv(\"/data01/kilm/users/quocvh/VAR/var_pdf/data/public_test_input/question.csv\")\n",
    "# df = pd.read_csv(f\"/data01/kilm/users/quocvh/VAR/var_pdf/submit2/qa_{SET}_search.csv\")\n",
    "# df = pd.read_csv(f\"/data01/kilm/users/quocvh/VAR/var_pdf/submit4/qa_{SET}_search_v2.csv\")\n",
    "# df = pd.read_csv(f\"/data01/kilm/users/quocvh/VAR/var_pdf/submit4/qa_{SET}_search_v6.csv\")\n",
    "df = pd.read_csv(f\"/data01/kilm/users/quocvh/VAR/var_pdf/submit4/qa_{SET}_search_v7.csv\")\n",
    "\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c83f12a9-a04a-4c10-8e7d-f0a8ee229ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d44817a-2e1e-4a7c-9376-19175a8de9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb0f21e6-9632-4ecf-ad7c-5ad9478013c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TEMPLATE =  \"\"\"Hãy chọn các đáp án đúng cho câu hỏi trắc nghiệm sau\n",
    "Ouput format: Các đáp án đúng cách nhau bởi dấu \",\", eg: A,B,C,D\n",
    "Chỉ trả về các chữ cái ABCD, không trả kèm gì thêm\n",
    "\n",
    "Câu hỏi: {Question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "\"\"\"\n",
    "\n",
    "TEMPLATE =  \"\"\"Hãy chọn các đáp án đúng cho câu hỏi trắc nghiệm sau dựa vào tài liệu\n",
    "Ouput format: Các đáp án đúng cách nhau bởi dấu \",\", eg: A,B,C,D\n",
    "Chỉ trả về các chữ cái ABCD, không trả kèm gì thêm\n",
    "\n",
    "Tài liệu: {context}\n",
    "\n",
    "Câu hỏi: {Question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "\"\"\"\n",
    "\n",
    "TEMPLATE =  \"\"\"Tài liệu: {context}\n",
    "\n",
    "Hãy chọn các đáp án đúng cho câu hỏi trắc nghiệm sau dựa vào tài liệu\n",
    "Ouput format: Các đáp án đúng cách nhau bởi dấu \",\", eg: A,B,C,D\n",
    "Chỉ trả về các chữ cái ABCD, không trả kèm gì thêm\n",
    "\n",
    "Câu hỏi: {Question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "\"\"\"\n",
    "\n",
    "TEMPLATE =  \"\"\"Tài liệu: {context}\n",
    "\n",
    "Hãy chọn các đáp án đúng cho câu hỏi trắc nghiệm sau dựa vào tài liệu\n",
    "Ouput format: \n",
    "Please show your choice in the answer field with only the choice letters, e.g., \"C\" or \"A,D\"\n",
    "\n",
    "Câu hỏi: {Question}\n",
    "A. {A}\n",
    "B. {B}\n",
    "C. {C}\n",
    "D. {D}\n",
    "\n",
    "Đáp án đúng là:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "x = data[0]\n",
    "\n",
    "\n",
    "# print(TEMPLATE.format(**x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f364555f-9a5c-4be1-b68a-ee561786fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "346639d3-5baf-41ff-bcaa-e8ff14dd0fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.decode(tokenizer.encode(\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46dab9c0-886a-49b4-b9da-7d0bf6a72580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-05 17:46:28 [chat_utils.py:473] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526716672b914d559e923237db30f6cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/310 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a78752cdf947979aa3dd51a88ff3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/310 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MM = []\n",
    "for x in data:\n",
    "    x[\"context\"] = tokenizer.decode(tokenizer.encode(x[\"context\"])[:30000])\n",
    "    prompt = TEMPLATE.format(**x)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    MM.append(messages)\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, top_k=1, max_tokens=10)\n",
    "outputs = llm.chat(MM, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7a7d5ce-0276-4928-be43-87a2bf681d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Không có thông tin nào trong tài liệu được c []\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "C ['C']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "A ['A']\n",
      "D ['D']\n",
      "B ['B']\n",
      "C ['C']\n",
      "A ['A']\n",
      "A,D ['A', 'D']\n",
      "Không có thông tin trong tài liệu cung cấp []\n",
      "D ['D']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "A ['A']\n",
      "C ['C']\n",
      "C ['C']\n",
      "C ['C']\n",
      "A ['A']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "D ['D']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "D ['D']\n",
      "B\n",
      "\n",
      "Giải thích:  \n",
      "- Tín ['B']\n",
      "A ['A']\n",
      "Không có thông tin trong tài liệu về việc sử []\n",
      "B ['B']\n",
      "A ['A']\n",
      "A ['A']\n",
      "A,D ['A', 'D']\n",
      "A ['A']\n",
      "A ['A']\n",
      "B ['B']\n",
      "Không có thông tin nào trong tài liệu về kích []\n",
      "Câu hỏi yêu cầu nội suy giá trị ['C']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "Không có thông tin trong tài liệu cung cấp []\n",
      "A ['A']\n",
      "A ['A']\n",
      "A ['A']\n",
      "A ['A']\n",
      "Không có thông tin nào trong tài liệu về môn []\n",
      "C ['C']\n",
      "B ['B']\n",
      "C ['C']\n",
      "Không có thông tin nào trong tài liệu về \" []\n",
      "A ['A']\n",
      "Không có thông tin nào trong tài liệu về môn []\n",
      "B ['B']\n",
      "A ['A']\n",
      "A,D ['A', 'D']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "Không có thông tin nào trong tài liệu được c []\n",
      "A,D ['A', 'D']\n",
      "C ['C']\n",
      "A ['A']\n",
      "B ['B']\n",
      "B ['B']\n",
      "D ['D']\n",
      "A ['A']\n",
      "C ['C']\n",
      "B ['B']\n",
      "D ['D']\n",
      "D ['D']\n",
      "Không có thông tin nào trong tài liệu về các []\n",
      "A ['A']\n",
      "C ['C']\n",
      "A ['A']\n",
      "A ['A']\n",
      "B ['B']\n",
      "C ['C']\n",
      "A ['A']\n",
      "C ['C']\n",
      "B ['B']\n",
      "D ['D']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "B ['B']\n",
      "C ['C']\n",
      "Không có thông tin nào trong tài liệu về dịch []\n",
      "B ['B']\n",
      "D ['D']\n",
      "B ['B']\n",
      "B ['B']\n",
      "Không có thông tin nào trong tài liệu về \" []\n",
      "A ['A']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "A ['A']\n",
      "B\n",
      "\n",
      "Giải thích:  \n",
      "Theo tài ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "C ['C']\n",
      "B ['B']\n",
      "A ['A']\n",
      "Không có thông tin nào trong tài liệu Public_ []\n",
      "B ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "C ['C']\n",
      "C ['C']\n",
      "B ['B']\n",
      "A ['A']\n",
      "D ['D']\n",
      "B ['B']\n",
      "C ['C']\n",
      "Không có thông tin nào trong tài liệu cung []\n",
      "C ['C']\n",
      "B ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "Không có thông tin trong tài liệu về \"lu []\n",
      "C ['C']\n",
      "A ['A']\n",
      "B ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "D ['D']\n",
      "A ['A']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "Không có thông tin trong tài liệu về bộ dụng []\n",
      "C ['C']\n",
      "B ['B']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "A ['A']\n",
      "A ['A']\n",
      "Không có thông tin nào trong tài liệu cung []\n",
      "D ['D']\n",
      "B ['B']\n",
      "Câu hỏi này yêu cầu xác định yêu cầu ['C']\n",
      "D ['D']\n",
      "C ['C']\n",
      "Không có thông tin nào trong tài liệu nói về []\n",
      "B ['B']\n",
      "A ['A']\n",
      "C ['C']\n",
      "B\n",
      "\n",
      "Giải thích:  \n",
      "Theo B ['B']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "B,C ['B', 'C']\n",
      "A ['A']\n",
      "Không có thông tin trong tài liệu về giới hạn []\n",
      "Không có thông tin nào trong tài liệu quy định []\n",
      "D ['D']\n",
      "B ['B']\n",
      "Không có thông tin nào trong tài liệu cung []\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "A ['A']\n",
      "A,C ['A', 'C']\n",
      "A ['A']\n",
      "B ['B']\n",
      "Không có thông tin trong tài liệu về \"Đ []\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "D ['D']\n",
      "B ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "Không có thông tin nào trong tài liệu về yêu []\n",
      "D ['D']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "A ['A']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A,C,D ['A', 'C', 'D']\n",
      "A,B ['A', 'B']\n",
      "Không có thông tin nào trong tài liệu được c []\n",
      "C ['C']\n",
      "B ['B']\n",
      "C ['C']\n",
      "A ['A']\n",
      "C ['C']\n",
      "B ['B']\n",
      "B. F M N F ['B']\n",
      "C ['C']\n",
      "C, D ['C', 'D']\n",
      "B ['B']\n",
      "A,C ['A', 'C']\n",
      "A ['A']\n",
      "B ['B']\n",
      "B. F ['B']\n",
      "A ['A']\n",
      "A ['A']\n",
      "D ['D']\n",
      "A ['A']\n",
      "B ['B']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "C ['C']\n",
      "C ['C']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "A ['A']\n",
      "A,B ['A', 'B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "Không có thông tin nào trong tài liệu được c []\n",
      "A ['A']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B. G , H , N ['B']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n",
      "C, D ['C', 'D']\n",
      "A ['A']\n",
      "B ['B']\n",
      "B ['B']\n",
      "C ['C']\n",
      "A ['A']\n",
      "B ['B']\n",
      "D ['D']\n",
      "A ['A']\n",
      "C ['C']\n",
      "B ['B']\n",
      "B ['B']\n",
      "Không có thông tin nào trong tài liệu `Public []\n",
      "C ['C']\n",
      "Không có thông tin nào trong tài liệu được c []\n",
      "Không có thông tin nào trong tài liệu về \" []\n",
      "B ['B']\n",
      "C ['C']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "A ['A']\n",
      "B ['B']\n",
      "B ['B']\n",
      "B ['B']\n",
      "D ['D']\n",
      "A ['A']\n",
      "Không có thông tin nào trong tài liệu cung []\n",
      "C ['C']\n",
      "Không có thông tin nào trong tài liệu được c []\n",
      "B ['B']\n",
      "C ['C']\n",
      "B ['B']\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for x, d in zip(outputs, data):\n",
    "    res = x.outputs[0].text\n",
    "    answers = []\n",
    "    for a in res.split(\",\"):\n",
    "        a = a.split(\".\")[0].strip()\n",
    "        \n",
    "        \n",
    "        if a:\n",
    "            a = a[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if a in \"ABCD\":\n",
    "            answers.append(a)\n",
    "\n",
    "    print(res, answers)\n",
    "\n",
    "\n",
    "    \n",
    "    if not answers:\n",
    "        preds.append(\"A\")\n",
    "        d[\"pred\"] = None\n",
    "    else:\n",
    "        preds.append(\",\".join(answers))\n",
    "        d[\"pred\"] = \",\".join(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e4668de-4d6f-48a8-91bd-c7e912462a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "md_paths = sorted(glob.glob(f\"/data01/kilm/users/quocvh/VAR/var_pdf/submit4/pymuf4llm_{SET}_output/*/*.md\"))\n",
    "md_docs = {}\n",
    "for i, path in enumerate(md_paths):\n",
    "    with open(path) as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # print(len(text.split()))\n",
    "\n",
    "    idx = int(path.split(\"/\")[-2].split(\"_\")[-1])\n",
    "    # idx2i[idx] = i\n",
    "    md_docs[idx] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f141e3-da14-4f16-b75a-c31fd08dff6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "26267229-e15b-42ed-a686-4820cdeb8bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "\n",
    "na_data = [x for x in data if not x[\"pred\"] ]\n",
    "len(na_data)\n",
    "for x in na_data:\n",
    "    # print(x[\"search\"])\n",
    "    idxs =  ast.literal_eval(x[\"search\"])\n",
    "    # print(idxs)\n",
    "    if len(idxs) <= 1:\n",
    "        x[\"pred\"] = \"A\"\n",
    "        continue\n",
    "    \n",
    "    idx =  int(idxs[9].split(\"_\")[-1])\n",
    "    # print(idx)\n",
    "    context = md_docs[idx]\n",
    "    x[\"context\"] = context\n",
    "\n",
    "na_data = [x for x in data if not x[\"pred\"] ]\n",
    "len(na_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db71971f-9325-4b38-8656-22fe1b205cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5a4bf8ad45424590cbd59efd5f8dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250a75161e2c454980e1ac1ecee249b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/6 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MM = []\n",
    "for x in na_data:\n",
    "    x[\"context\"] = tokenizer.decode(tokenizer.encode(x[\"context\"])[:30000])\n",
    "    prompt = TEMPLATE.format(**x)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    MM.append(messages)\n",
    "\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.0, top_k=1, max_tokens=10)\n",
    "outputs = llm.chat(MM, sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "20b74339-4d5e-4a00-8b56-dc80d5b4dabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Không có thông tin nào trong tài liệu cung []\n",
      "Không có thông tin nào trong tài liệu về môn []\n",
      "Không có thông tin nào trong tài liệu về \" []\n",
      "Không có thông tin nào trong tài liệu được c []\n",
      "Không có thông tin nào trong tài liệu về \" []\n",
      "Không có thông tin nào trong tài liệu cung []\n"
     ]
    }
   ],
   "source": [
    "# preds = []\n",
    "for x, d in zip(outputs, na_data):\n",
    "    res = x.outputs[0].text\n",
    "    answers = []\n",
    "    for a in res.split(\",\"):\n",
    "        a = a.split(\".\")[0].strip()\n",
    "        \n",
    "        \n",
    "        if a:\n",
    "            a = a[0]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        if a in \"ABCD\":\n",
    "            answers.append(a)\n",
    "\n",
    "    print(res, answers)\n",
    "    \n",
    "    if not answers:\n",
    "        # preds.append(\"A\")\n",
    "        d[\"pred\"] = None\n",
    "    else:\n",
    "        # preds.append(\",\".join(answers))\n",
    "        d[\"pred\"] = \",\".join(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "335cd7c9-fe22-4edd-972a-9cb72111f66e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# na_data[0]\n",
    "\n",
    "preds = []\n",
    "for x in data:\n",
    "    if x[\"pred\"]:\n",
    "        preds.append(x[\"pred\"])\n",
    "    else:\n",
    "        preds.append(\"A\")\n",
    "len(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1160a-f236-49cd-9c66-c59342034ea4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "22be6b9b-4c79-4f94-b53f-9b2ded10b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"num_correct\"] = [len(x.split(\",\")) for x in preds]\n",
    "df[\"answers\"] = preds\n",
    "# df.to_csv(f\"/data01/kilm/users/quocvh/VAR/var_pdf/submit2/qa_{SET}.csv\", encoding=\"utf-8-sig\", index=False)\n",
    "df.to_csv(f\"/data01/kilm/users/quocvh/VAR/var_pdf/submit4/qa_{SET}_v10.csv\", encoding=\"utf-8-sig\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e4ede-afca-4190-a45c-6765c8feb690",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quocvh2",
   "language": "python",
   "name": "quocvh2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
